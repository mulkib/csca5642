{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_0"
      },
      "source": [
        "# 0. Ready for Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_kaggle"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c gan-getting-started\n",
        "!unzip gan-getting-started.zip -d /content/gan/"
      ],
      "metadata": {
        "id": "Lmzsqx7HOaEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(20211001)\n",
        "tf.random.set_seed(20211001)"
      ],
      "metadata": {
        "id": "ky__iNjVOUpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_1"
      },
      "source": [
        "# 1. Problem & Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem_description"
      },
      "source": [
        "## 1.1 Problem Definition\n",
        "\n",
        "* The problem is to build a GAN model that transforms a general photograph into the artistic style of Claude Monet while preserving the structure of the original photo.\n",
        "\n",
        "* A GAN (Generative Adversarial Network) is an architecture where two neural networks, a Generator and a Discriminator, learn by competing against each other.\n",
        "\n",
        " * The Generator creates realistic-looking fake data.\n",
        "\n",
        " * The Discriminator is trained to distinguish between the fake data created by the Generator and real data, with the goal of differentiating real Monet paintings from the fake data.\n",
        "\n",
        " * Why is it an 'Adversarial' Network? Because the Generator continuously creates more realistic data to fool the Discriminator, and the Discriminator, in turn, gets better at telling real from fake to avoid being fooled. As this 'competition' repeats, the Generator becomes capable of producing highly realistic results.**(\"Korean-To-English Translation\")**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. Data Description\n",
        "\n",
        "* Data Format: Provided in two formats: TFRecord (.tfrec) and JPEG (.jpg).\n",
        "\n",
        "* Dimensions: 256x256 pixels with 3 color channels (RGB).\n",
        "\n",
        "* Data Structure and Size:\n",
        "\n",
        " * monet_jpg / monet_tfrec: 300 images (for training)\n",
        "\n",
        " * photo_jpg / photo_tfrec: 7,038 images (photos to be transformed into the Monet style)**(\"Korean-To-English Translation\")**"
      ],
      "metadata": {
        "id": "8BbOyNf6NhE7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_2"
      },
      "source": [
        "# 2. EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "data_description"
      },
      "source": [
        "MONET_FILENAMES = tf.io.gfile.glob('/content/gan/monet_tfrec/*.tfrec')\n",
        "PHOTO_FILENAMES = tf.io.gfile.glob('/content/gan/photo_tfrec/*.tfrec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "IMAGE_SIZE = [256, 256]\n",
        "BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1.0\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    tfrecord_format = {'image': tf.io.FixedLenFeature([], tf.string)}\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = decode_image(example['image'])\n",
        "    return image\n",
        "\n",
        "def load_dataset(filenames):\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "monet_dataset = load_dataset(MONET_FILENAMES)\n",
        "photo_dataset = load_dataset(PHOTO_FILENAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_monet_samples = len(list(monet_dataset.unbatch().take(300)))"
      ],
      "metadata": {
        "id": "R2h1-0UPPr28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visualize_samples"
      },
      "source": [
        "def display_images(dataset, n=5, title=\"Images\"):\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i, image in enumerate(dataset.take(n)):\n",
        "        if i >= n:\n",
        "            break\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.imshow((image[0] + 1) / 2)\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "display_images(monet_dataset, n=5, title=\"Sample Monet Paintings\")\n",
        "display_images(photo_dataset, n=5, title=\"Sample Photos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_3"
      },
      "source": [
        "# 3. Architecture\n",
        "\n",
        "* Generator : It begins with a 100-dimensional random noise vector, which is reshaped into an 8x8 image. It then repeatedly uses Conv2DTranspose layers to upsample the image, ultimately creating a 256x256 color image.\n",
        "\n",
        " * BatchNorm : This normalizes the input data for each layer to have a mean of 0 and a variance of 1. It helps solve the \"Internal Covariate Shift\" problem, which occurs because the distribution of each layer's input data changes unstably as the weights of previous layers are updated during training.\n",
        "\n",
        " * LeakyReLU : The standard ReLU function, max(0, x), causes the \"Dying ReLU\" problem where learning stops because the gradient becomes zero for negative inputs. LeakyReLU fixes this by multiplying negative inputs by a very small constant instead of outputting zero.\n",
        "\n",
        "* Discriminator : It takes a 256x256 image (real or fake) as input and uses a series of Conv2D layers to continuously reduce the image size while extracting key features. Finally, it outputs the probability of the image being real. **(\"Korean-To-English Translation\")**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generator_architecture"
      },
      "source": [
        "LATENT_DIM = 100\n",
        "\n",
        "def build_generator():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(8 * 8 * 512, input_shape=(LATENT_DIM,)),\n",
        "        layers.Reshape((8, 8, 512)),\n",
        "\n",
        "        layers.Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2DTranspose(32, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2DTranspose(16, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2D(3, kernel_size=5, padding='same', activation='tanh')\n",
        "    ], name='generator')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "discriminator_architecture"
      },
      "source": [
        "def build_discriminator():\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, kernel_size=5, strides=2, padding='same', input_shape=[256, 256, 3]),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(64, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(128, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(256, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(512, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ], name='discriminator')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "build_models"
      },
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "generator.summary()\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loss_optimizers"
      },
      "source": [
        "cross_entropy = keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training_functions"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow((predictions[i] + 1) / 2)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f'Generated Images at Epoch {epoch}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training_loop"
      },
      "source": [
        "EPOCHS = 100\n",
        "num_examples_to_generate = 16\n",
        "D_TRAIN_FREQ = 5\n",
        "DISC_LOSS_THRESHOLD = 0.5\n",
        "\n",
        "steps_per_epoch = n_monet_samples // BATCH_SIZE\n",
        "seed = tf.random.normal([num_examples_to_generate, LATENT_DIM])\n",
        "\n",
        "gen_losses = []\n",
        "disc_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    gen_loss_epoch = []\n",
        "    disc_loss_epoch = []\n",
        "\n",
        "    progbar = keras.utils.Progbar(steps_per_epoch)\n",
        "\n",
        "    for i, image_batch in enumerate(monet_dataset.take(steps_per_epoch)):\n",
        "        train_discriminator = True\n",
        "\n",
        "        if i % D_TRAIN_FREQ != 0:\n",
        "            train_discriminator = False\n",
        "\n",
        "        if len(disc_loss_epoch) > 0 and np.mean(disc_loss_epoch[-5:]) < DISC_LOSS_THRESHOLD:\n",
        "            train_discriminator = False\n",
        "\n",
        "        noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_images = generator(noise, training=True)\n",
        "\n",
        "            real_output = discriminator(image_batch, training=True)\n",
        "            fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "            gen_loss = generator_loss(fake_output)\n",
        "            disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "        if train_discriminator:\n",
        "            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "        gen_loss_epoch.append(gen_loss)\n",
        "        disc_loss_epoch.append(disc_loss)\n",
        "        progbar.update(i+1)\n",
        "\n",
        "    gen_losses.append(np.mean(gen_loss_epoch))\n",
        "    disc_losses.append(np.mean(disc_loss_epoch))\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        generate_and_save_images(generator, epoch + 1, seed)\n",
        "        generator.save_weights(f'/tmp/generator_epoch_{epoch+1}.weights.h5')\n",
        "        discriminator.save_weights(f'/tmp/discriminator_epoch_{epoch+1}.weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_4"
      },
      "source": [
        "# 4. Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training_visualization"
      },
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(gen_losses, label='Generator Loss', linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Generator Loss During Training', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(disc_losses, label='Discriminator Loss', color='orange', linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Discriminator Loss During Training', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_samples"
      },
      "source": [
        "n_samples = 25\n",
        "sample_noise = tf.random.normal([n_samples, LATENT_DIM])\n",
        "generated_samples = generator(sample_noise, training=False)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "for i in range(n_samples):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    img = (generated_samples[i] + 1) / 2\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('Final Generated Monet-Style Images', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compare_real_fake"
      },
      "source": [
        "def compare_real_vs_generated(real_dataset, generator, n=5):\n",
        "    fig, axes = plt.subplots(2, n, figsize=(15, 6))\n",
        "\n",
        "    for i, real_img in enumerate(real_dataset.take(n)):\n",
        "        axes[0, i].imshow((real_img[0] + 1) / 2)\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[0, i].set_ylabel('Real Monet', fontsize=14)\n",
        "\n",
        "    noise = tf.random.normal([n, LATENT_DIM])\n",
        "    generated = generator(noise, training=False)\n",
        "    for i in range(n):\n",
        "        axes[1, i].imshow((generated[i] + 1) / 2)\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[1, i].set_ylabel('Generated', fontsize=14)\n",
        "\n",
        "    plt.suptitle('Real Monet Paintings vs Generated Images', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_real_vs_generated(monet_dataset, generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diversity_analysis"
      },
      "source": [
        "base_noise = tf.random.normal([1, LATENT_DIM])\n",
        "variations = []\n",
        "\n",
        "for i in range(9):\n",
        "    variation = base_noise + tf.random.normal([1, LATENT_DIM]) * 0.1\n",
        "    variations.append(variation)\n",
        "\n",
        "variations = tf.concat(variations, axis=0)\n",
        "generated_variations = generator(variations, training=False)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow((generated_variations[i] + 1) / 2)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('Diversity Test: Variations from Similar Noise Vectors', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_5"
      },
      "source": [
        "# 5. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "* Results and Interpretation\n",
        "\n",
        " * The Generator Loss oscillates between 0.4 and 1.8, while the Discriminator Loss rises sharply until epoch 20 and then fluctuates widely between 4 and 7.\n",
        "\n",
        " * A high Discriminator Loss when the Generator Loss is low is a classic sign of the adversarial training characteristic in GANs.\n",
        "\n",
        " * The generated images did not come close to the artistic style of Claude Monet.\n",
        "\n",
        "* Shortcomings\n",
        "\n",
        " * The Discriminator Loss shows high volatility, fluctuating between 4 and 7, which indicates unoptimized training. Furthermore, even after 100 epochs, the loss fails to converge or stabilize, continuing to oscillate. This suggests a need to increase the number of epochs or, if it still fails to stabilize, to introduce a new GAN architecture, improve the Generator and Discriminator, or implement a new loss function.\n",
        "\n",
        " * The evaluation relies on subjective visual assessment. However, human perception is subjective; an image might be quantitatively closer to Monet's style even if it doesn't appear so. Therefore, it's necessary to introduce objective metrics like MiFiD, as suggested by the Kaggle Dataset.\n",
        "\n",
        "* Future Improvements\n",
        "\n",
        " * Currently, the Discriminator is trained only once every five steps, which causes large fluctuations in the loss. The training frequency could be dynamically adjusted. For instance, if the Discriminator Loss exceeds 6, it could be trained every step, and if it drops below 2, it could be trained only once every 10 steps.\n",
        "\n",
        " * Binary Cross-Entropy can cause the gradient vanishing problem in the early stages of GAN training (Goodfellow et al., 2014). Introducing the Wasserstein Loss, as proposed by Arjovsky et al. (2017) to solve such training instabilities, could mitigate this problem.\n",
        "\n",
        " * It seems necessary to adopt the CycleGAN architecture, as introduced in the Kaggle Dataset.**(\"Korean-To-English Translation\")**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_6"
      },
      "source": [
        "# 6. Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_submission"
      },
      "source": [
        "os.makedirs('/tmp/generated_images', exist_ok=True)\n",
        "\n",
        "num_images = 7500\n",
        "batch_size = 50\n",
        "num_batches = num_images // batch_size\n",
        "\n",
        "for batch in range(num_batches):\n",
        "    noise = tf.random.normal([batch_size, LATENT_DIM])\n",
        "    generated_images = generator(noise, training=False)\n",
        "\n",
        "    for i, img in enumerate(generated_images):\n",
        "        img_array = ((img.numpy() + 1) * 127.5).astype(np.uint8)\n",
        "        img_pil = Image.fromarray(img_array)\n",
        "        img_pil.save(f'/tmp/generated_images/monet_{batch * batch_size + i:05d}.jpg', quality=95)\n",
        "\n",
        "with zipfile.ZipFile('images.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for i in range(num_images):\n",
        "        filename = f'monet_{i:05d}.jpg'\n",
        "        zipf.write(f'/tmp/generated_images/{filename}', filename)\n",
        "\n",
        "import shutil\n",
        "shutil.rmtree('/tmp/generated_images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "verify_submission"
      },
      "source": [
        "with zipfile.ZipFile('images.zip', 'r') as zipf:\n",
        "    file_list = zipf.namelist()\n",
        "    with zipf.open(file_list[0]) as img_file:\n",
        "        img = Image.open(img_file)\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Sample Submission Image')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('images.zip')"
      ],
      "metadata": {
        "id": "5Al7-1HAU7w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. References\n",
        "\n",
        "* Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative adversarial nets. Advances in Neural Information Processing Systems, 27, 2672-2680.\n",
        "\n",
        "* Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein generative adversarial networks. Proceedings of the 34th International Conference on Machine Learning, 70, 214-223.\n",
        "\n",
        "* \"Korean-To-English Translation\" prompt. Gemini, Google, 29 July 2025, https://g.co/gemini/share/8d0a80f6bc41."
      ],
      "metadata": {
        "id": "IORRIoUJVbQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Github\n",
        "\n",
        "- https://github.com/mulkib/csca5642.git"
      ],
      "metadata": {
        "id": "DGI-8WE60I-z"
      }
    }
  ]
}